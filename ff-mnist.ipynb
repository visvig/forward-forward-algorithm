{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a20b3aa",
   "metadata": {},
   "source": [
    "# Implementation of Hinton's Forward-Forward Algorithm\n",
    "\n",
    "This notebook contains an implementation of Geoffrey Hinton's Forward-Forward Algorithm. Details about this algorithm can be found in Hinton's [paper](https://arxiv.org/abs/2212.13345#:~:text=The%20Forward%2DForward%20algorithm%20replaces,generated%20by%20the%20network%20itself).\n",
    "\n",
    "## Preparation:\n",
    "\n",
    "- **Import Libraries:** Set up the coding environment with the required libraries.\n",
    "\n",
    "## Definition of Necessary Functions and Classes:\n",
    "\n",
    "- **A. Function to Generate Negative Labels:** Generate negative samples for training.\n",
    "\n",
    "- **B. Function to Overlay Label onto the Input Data:** Overlay the correct class onto the input data for positive forward pass.\n",
    "\n",
    "- **C. Custom Net Class:** Definition of the network using the custom forward-forward approach.\n",
    "\n",
    "- **D. Custom Layer Class:** Definition of a custom layer class to be used in the network.\n",
    "\n",
    "- **E. Network Hyperparameters and Device Setup:** Set the hyperparameters for the network and configure the computation device.\n",
    "\n",
    "## Implementation Steps:\n",
    "\n",
    "- **1. Load Data:** Load the MNIST dataset for training and testing.\n",
    "\n",
    "- **2. Create Network:** Instantiate the network architecture.\n",
    "\n",
    "- **3. Train the Network:** Train the network using the forward-forward algorithm.\n",
    "\n",
    "- **4. Test the Network:** Test the network's performance on the MNIST test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a00b88",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fcd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn  \n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda \n",
    "from torch.utils.data import DataLoader \n",
    "from torch.optim import Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a8473",
   "metadata": {},
   "source": [
    "### A. Function to generate negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c983fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates negative labels for the training data, which are required for contrastive divergence training\n",
    "\n",
    "def get_y_neg(y):\n",
    "\n",
    "    # clone the original labels\n",
    "    y_neg = y.clone()\n",
    "\n",
    "    # loop through each sample in the labels\n",
    "    for idx, y_samp in enumerate(y):\n",
    "        \n",
    "        # create a list of all possible class labels\n",
    "        allowed_indices = list(range(10))\n",
    "\n",
    "        # remove the correct label from the list, leaving only incorrect labels\n",
    "        allowed_indices.remove(y_samp.item())\n",
    "\n",
    "        # assigning a random label from allowed_indices to y_neg[idx].\n",
    "        y_neg[idx] = torch.tensor(allowed_indices)[torch.randint(len(allowed_indices), size=(1,))].item()\n",
    "\n",
    "    # return the labels with incorrect class, moved to the specified device (CPU or GPU)\n",
    "    return y_neg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8c612",
   "metadata": {},
   "source": [
    "### B. Function to overlay label onto the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08fc036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlays label info onto the input data, making a certain position in the tensor representative of the label\n",
    "\n",
    "def overlay_y_on_x(x, y, classes=10):\n",
    "    \n",
    "    # Create a copy of the input tensor x\n",
    "    x_ = x.clone()\n",
    "\n",
    "    # for all samples, set the first 10 column values to 0\n",
    "    x_[:, :classes] *= 0.0\n",
    "\n",
    "    # For each sample, set the feature at index corresponding to the class y to be the max value in x\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "\n",
    "    # Return the transformed tensor\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068aa2b",
   "metadata": {},
   "source": [
    "### C. Custom Net Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19087c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Net class that inherits from torch.nn.Module which is the base class for all neural network modules in PyTorch.\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    # Initialize the Net class with dimensions of the neural network layers\n",
    "    def __init__(self, dims):\n",
    "\n",
    "        # Initialize the parent class torch.nn.Module, Net is the subclass and torch.nn.Module is the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize an empty layers list that will store the layers of the neural network\n",
    "        self.layers = []\n",
    "\n",
    "        # Create the layers of the neural network\n",
    "        for d in range(len(dims) - 1):\n",
    "            \n",
    "            # Append a new layer to the layers list and move it to the appropriate device\n",
    "            self.layers = self.layers + [Layer(dims[d], dims[d + 1]).to(device)]\n",
    "\n",
    "    # Define a function to make predictions using the neural network\n",
    "    def predict(self, x):\n",
    "\n",
    "        # This list will store a measure of how much the input tensor x looks like each possible digit (from 0 to 9).\n",
    "        goodness_per_label = []\n",
    "\n",
    "        # For each possible label (0 to 9)\n",
    "        for label in range(10):\n",
    "\n",
    "            # Apply the label overlay to the input to make h tensor (50000, 784) of same size as x \n",
    "            h = overlay_y_on_x(x, label)\n",
    "\n",
    "            '''\n",
    "            h is 2D tensor of shape (50000, 784):\n",
    "\n",
    "            h = \n",
    "                tensor([[0.0, 0.0, ......., 0.7],   # sample 1\n",
    "                        [0.0, 0.0, ......., 0.6],   # sample 2\n",
    "                        [0.0, 0.0, ......., 0.7],   # sample 3\n",
    "                        [0.0, 1.0, ......., 0.7],   # sample 4\n",
    "                        [0.0, 0.0, ......., 0.7],   # sample 5\n",
    "                        ...\n",
    "                        [0.0, 0.0, ......., 0.7],   # sample 49,996\n",
    "                        [1.0, 0.0, ......., 0.7],   # sample 49,997\n",
    "                        [0.0, 0.0, ......., 0.7],   # sample 49,998\n",
    "                        [1.0, 0.0, ......., 0.7],   # sample 49,999\n",
    "                        [0.0, 1.0, ......., 0.7]])  # sample 50,000          \n",
    "                '''\n",
    "\n",
    "            # Create a list to store the goodness of each layer for this label\n",
    "            goodness = []\n",
    "\n",
    "            # Pass the input through each Layer class element in layers list. \n",
    "            for layer in self.layers:\n",
    "                \n",
    "                # Apply the current layer to the transformed input tensor h, and then stores the result back into h.\n",
    "                h = layer(h)\n",
    "\n",
    "                # after layer 1, h size: (50000, 500)\n",
    "                # after layer 2, h size: (50000, 500)\n",
    "\n",
    "                # Add the squared mean (mean along each rows) of the output of this layer to the goodness list\n",
    "                goodness = goodness + [h.pow(2).mean(1)]\n",
    "\n",
    "            '''\n",
    "            goodness_of_a_layer is 1D tensor of shape (50000,):\n",
    "\n",
    "            goodness_1 = [\n",
    "                tensor([0.1, 0.2, ...................................., 0.1]),  # 50,000 mean squared activations for layer 1\n",
    "            ]\n",
    "            goodness_2 = [\n",
    "                tensor([0.1, 0.2, ...................................., 0.1]),  # 50,000 mean squared activations for layer 2\n",
    "            ]\n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "            goodness is a list of 1D tensors. list size is (50000,2):\n",
    "\n",
    "            goodness = [\n",
    "                tensor([0.1, 0.2, ...................................., 0.1]),  # 50,000 mean squared activations for layer 1\n",
    "                tensor([0.3, 0.2, ...................................., 0.2])   # 50,000 mean squared activations for layer 2\n",
    "            ]\n",
    "            '''\n",
    "\n",
    "            # Add the sum of the goodness of all layers for this label to the goodness_per_label list\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "\n",
    "            '''\n",
    "            sum(goodness) is 1D tensor of shape (50000,):\n",
    "\n",
    "            sum_of_goodness = tensor([0.4, 0.4, ...................................., 0.3]) # 50,000 sums of mean squared activations\n",
    "\n",
    "            sum(goodness).unsqueeze(1) then adds an extra dimension to this tensor, \n",
    "            It is 2D tensor of shape (50000, 1):\n",
    "\n",
    "            sum_of_goodness_unsqueezed = \n",
    "                tensor([[0.4], [0.4], ...................................., [0.3]])     \n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "            This tensor is then added to the goodness_per_label list. For 10 lables, goodness_per_label might look something like this after 10 iterations:\n",
    "            size of list is 10, each of 2D tensor shape (50000, 1):\n",
    "            \n",
    "            goodness_per_label = [\n",
    "                tensor([[0.6], [0.5], ...................................., [0.6]]),   # sum of goodness for label 0\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 1\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 2\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 3\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 4\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 5\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 6\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 7\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]]),   # sum of goodness for label 8\n",
    "                tensor([[0.7], [0.6], ...................................., [0.7]])    # sum of goodness for label 9\n",
    "                \n",
    "            ]\n",
    "            '''\n",
    "\n",
    "        # Concatenate all the goodness_per_label tensors along dimension 1\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "\n",
    "        '''\n",
    "        tensor of shape (50000, 10):\n",
    "\n",
    "        goodness_per_label = \n",
    "            tensor([[0.6, 0.7, ..., 0.7],   # goodness for sample 1\n",
    "                    [0.5, 0.6, ..., 0.6],   # goodness for sample 2\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 3\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 4\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 5\n",
    "                    ...\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 49,996\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 49,997\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 49,998\n",
    "                    [0.7, 0.6, ..., 0.7],   # goodness for sample 49,999\n",
    "                    [0.6, 0.7, ..., 0.7]])  # goodness for sample 50,000\n",
    "                \n",
    "        '''\n",
    "\n",
    "        # Return the label with the highest goodness\n",
    "        return goodness_per_label.argmax(1)\n",
    "    \n",
    "        '''\n",
    "        return the indices of maximum values along dimension 1 (columns):\n",
    "        1D tensor of shape (50000,):\n",
    "\n",
    "        tensor([1, 9, ...................................., 1]) \n",
    "        '''\n",
    "\n",
    "    # Define a function to train the neural network\n",
    "    def train(self, x_pos, x_neg):\n",
    "\n",
    "        # Initialize the positive and negative inputs\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "\n",
    "        # For each layer in the neural network\n",
    "        for i, layer in enumerate(self.layers):\n",
    "\n",
    "            # Print which layer is currently being trained\n",
    "            print(\"training layer: \", i)\n",
    "\n",
    "            # Train the layer using the positive and negative inputs\n",
    "            # layer.train() returns the positive and negative outputs of the layer which are 2d tensors of shape (50000, 500) for layer 1 and (50000, 500) for layer 2\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a1dcc",
   "metadata": {},
   "source": [
    "### D. Custom Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08670bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net is the main network and contains several instances of Layer, which represents individual layers of the network.\n",
    "class Layer(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
    "\n",
    "        # Initialize the parent class, torch.nn.Linear\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "\n",
    "        # Define a ReLU (Rectified Linear Unit) activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        # Define an Adam optimizer for this layer, with the learning rate defined by args.lr\n",
    "        self.opt = Adam(self.parameters(), lr=args.lr)\n",
    "\n",
    "        '''\n",
    "        learning rate = 0.3\n",
    "        '''\n",
    "\n",
    "        # Define a threshold for the loss function, specified by args.threshold\n",
    "        self.threshold = args.threshold\n",
    "\n",
    "        '''\n",
    "        3\n",
    "        '''\n",
    "\n",
    "        # Define the number of training epochs for this layer, specified by args.epochs\n",
    "        self.num_epochs = args.epochs\n",
    "\n",
    "        '''\n",
    "        1000\n",
    "        '''\n",
    "\n",
    "    # Make each sample as unit magnitude then do A = X*W.T + B\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Normalize the input tensor along each row\n",
    "        # Each row in x_direction is a unit vector in the direction of the corresponding row in x\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "\n",
    "        '''\n",
    "        x_direction is 2D tensor of shape (50000, 784):\n",
    "\n",
    "        x_direction = \n",
    "            tensor([[0.14, 0.25, ......., 0.09],   # sample 1\n",
    "                    [0.32, 0.51, ......., 0.72],   # sample 2\n",
    "                    [0.85, 0.17, ......., 0.37],   # sample 3\n",
    "                    [0.92, 0.45, ......., 0.88],   # sample 4\n",
    "                    [0.12, 0.93, ......., 0.76],   # sample 5\n",
    "                    ...\n",
    "                    [0.41, 0.18, ......., 0.14],   # sample 49,996\n",
    "                    [0.36, 0.74, ......., 0.59],   # sample 49,997\n",
    "                    [0.95, 0.21, ......., 0.86],   # sample 49,998\n",
    "                    [0.73, 0.41, ......., 0.32],   # sample 49,999\n",
    "                    [0.58, 0.66, ......., 0.09]])  # sample 50,000         \n",
    "            '''\n",
    "        \n",
    "        # Compute the linear transformation followed by a ReLU activation\n",
    "        # weight.T is 2D tensor of shape (784, 500)\n",
    "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
    "    \n",
    "        '''\n",
    "        weight is a 2D tensor of shape (500, 784):\n",
    "\n",
    "        weight = \n",
    "            tensor([[0.14, 0.25, ......., 0.09],   # node 1\n",
    "                    [0.32, 0.51, ......., 0.72],   # node 2\n",
    "                    [0.85, 0.17, ......., 0.37],   # node 3\n",
    "                    [0.92, 0.45, ......., 0.88],   # node 4\n",
    "                    [0.12, 0.93, ......., 0.76],   # node 5\n",
    "                    .\n",
    "                    .\n",
    "                    .\n",
    "                    [0.41, 0.18, ......., 0.14],   # node 496\n",
    "                    [0.36, 0.74, ......., 0.59],   # node 497\n",
    "                    [0.95, 0.21, ......., 0.86],   # node 498\n",
    "                    [0.73, 0.41, ......., 0.32],   # node 499\n",
    "                    [0.58, 0.66, ......., 0.09]])  # node 500\n",
    "\n",
    "        bias is a 1D tensor of shape (500,):\n",
    "\n",
    "        bias = tensor([0.1, 0.2, 0.3, ..., 0.498, 0.499, 0.500])\n",
    "\n",
    "        bias.unsqueeze(0) is a 2D tensor of shape (1,500)\n",
    "\n",
    "        bias_unsqueezed = tensor([[0.1, 0.2, 0.3, ..., 0.498, 0.499, 0.500]])\n",
    "        '''\n",
    "\n",
    "    # train a layer\n",
    "    def train(self, x_pos, x_neg):\n",
    "\n",
    "        # For each layer epoch \n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "            # Compute the goodness for the positive and negative samples\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "\n",
    "            '''\n",
    "            g_pos is 1D tensor of shape (50000,):\n",
    "            g_pos = tensor([0.7, 0.9, ...................................., 0.8])\n",
    "\n",
    "            '''\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "\n",
    "            '''\n",
    "            g_neg is 1D tensor of shape (50000,):\n",
    "            g_neg = tensor([0.1, 0.2, ...................................., 0.1])\n",
    "\n",
    "            '''\n",
    "            \n",
    "            # Compute the loss using the computed goodness and the specified threshold\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
    "\n",
    "            '''\n",
    "            torch.cat([-g_pos + self.threshold, g_neg - self.threshold]) \n",
    "            is 1D tensor of shape (100000,)\n",
    "\n",
    "            torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold])) \n",
    "            to ensure that all values are positive and to increase the difference between larger and smaller values\n",
    "            is 1D tensor of shape (100000,)\n",
    "\n",
    "            '1 + ...' adds 1 to each of these exponential scores. This ensures that even if all the scores are zero, the loss will not be negative.\n",
    "            still a 1D tensor of shape (100000,)\n",
    "\n",
    "            log()\n",
    "            still a 1D tensor of shape (100000,)\n",
    "\n",
    "            .mean() to take avg of all 100000 values\n",
    "            loss is scalar\n",
    "\n",
    "            '''\n",
    "\n",
    "            # Zero the gradients\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            '''\n",
    "            After self.opt.zero_grad() is called: The gradients of weights and biases are now zero\n",
    "\n",
    "            weight is a 2D tensor of shape (500, 784) for layer 1 and (500,500) for layer 2:\n",
    "\n",
    "            self.weight.grad = tensor([[0.00, 0.00, ...., 0.00],   # node 1\n",
    "                                       [0.00, 0.00, ...., 0.00],   # node 2\n",
    "                                       ...\n",
    "                                       [0.00, 0.00, ...., 0.00],   # node 499\n",
    "                                       [0.00, 0.00, ...., 0.00]])  # node 500\n",
    "\n",
    "            bias is a 1D tensor of shape (500,) for layer 1 and (500,) for layer 2:\n",
    "\n",
    "            self.bias.grad = tensor([0.00, 0.00, ..., 0.00, 0.00]) \n",
    "            \n",
    "            '''\n",
    "\n",
    "            # Compute the backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            '''\n",
    "            Assume the gradients of weights and biases after backpropagation are:\n",
    "\n",
    "            weight is a 2D tensor of shape (500, 784) for layer 1 and (500,500) for layer 2:\n",
    "            \n",
    "            self.weight.grad = tensor([[0.01, 0.02, ...., 0.02],   # node 1\n",
    "                                       [0.03, 0.01, ...., 0.01],   # node 2\n",
    "                                       ...\n",
    "                                       [0.01, 0.02, ...., 0.03],   # node 499\n",
    "                                       [0.02, 0.01, ...., 0.01]])  # node 500\n",
    "\n",
    "            bias is a 1D tensor of shape (500,) for layer 1 and (500,) for layer 2:\n",
    "\n",
    "            self.bias.grad = tensor([0.01, 0.02, ..., 0.02, 0.01])  \n",
    "            \n",
    "            '''\n",
    "\n",
    "            # Perform a step of optimization\n",
    "            self.opt.step()\n",
    "\n",
    "            '''\n",
    "            After opt.step(): weight and bias has been updated\n",
    "\n",
    "            weight is a 2D tensor of shape (500, 784) for layer 1 and (500, 500) for layer 2:\n",
    "            weight = \n",
    "                tensor([[0.13, 0.26, ......., 0.08],   # node 1\n",
    "                        [0.31, 0.52, ......., 0.73],   # node 2\n",
    "                        [0.86, 0.16, ......., 0.38],   # node 3\n",
    "                        [0.93, 0.44, ......., 0.89],   # node 4\n",
    "                        [0.11, 0.94, ......., 0.75],   # node 5\n",
    "                        .\n",
    "                        .\n",
    "                        .\n",
    "                        [0.42, 0.17, ......., 0.13],   # node 496\n",
    "                        [0.35, 0.75, ......., 0.58],   # node 497\n",
    "                        [0.96, 0.20, ......., 0.87],   # node 498\n",
    "                        [0.74, 0.42, ......., 0.33],   # node 499\n",
    "                        [0.59, 0.65, ......., 0.10]])  # node 500\n",
    "\n",
    "            bias is a 1D tensor of shape (500,) for layer 1 and (500,) for layer 2:\n",
    "\n",
    "            bias = \n",
    "                tensor([0.099, 0.201, 0.298, ..., 0.498, 0.501, 0.500])\n",
    "            '''\n",
    "\n",
    "            # Print the loss\n",
    "            if i % args.log_interval == 0:\n",
    "                print(\"Loss: \", loss.item())\n",
    "\n",
    "        # compute the output of the layer (A) for both positive and negative samples\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4e84c",
   "metadata": {},
   "source": [
    "### E. Network Hyperparameters and device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21607cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train_size = 50000\n",
    "    test_size = 10000\n",
    "    epochs = 1000\n",
    "    lr = 0.03\n",
    "    no_cuda = False\n",
    "    no_mps = False\n",
    "    save_model = False \n",
    "    threshold = 2\n",
    "    seed = 1234\n",
    "    log_interval = 10\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {\"batch_size\": args.train_size} # dictionary specifying train batch size\n",
    "test_kwargs = {\"batch_size\": args.test_size} # dictionary specifying test batch size\n",
    "\n",
    "# If CUDA is available\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93122d",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4475322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Converted to a tensor. MNIST are PIL images, so convert them to tensors to use with PyTorch\n",
    "# 2) Normalized for the MNIST dataset\n",
    "# 3) Flattened to a 1D tensor of 784 elements (28x28 pixels)\n",
    "transform = Compose( \n",
    "    [\n",
    "        ToTensor(),                                                                   \n",
    "        Normalize((0.1307,), (0.3081,)),       \n",
    "        Lambda(lambda x: torch.flatten(x)),    \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create train and test DataLoader instances using the MNIST dataset\n",
    "train_loader = DataLoader(MNIST(\"./data/\", train=True, download=True, transform=transform), **train_kwargs) \n",
    "test_loader = DataLoader(MNIST(\"./data/\", train=False, download=True, transform=transform), **test_kwargs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ff2de",
   "metadata": {},
   "source": [
    "### 2. Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2309a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Net class with an input dim of 784 neurons to match the flattened 28x28 pixel MNIST images\n",
    "# and two intermediate dims of 500 each. 3 dims so we will have a 2 layer network.\n",
    "net =  Net([784, 500, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9372db",
   "metadata": {},
   "source": [
    "### 3. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e44dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer:  0\n",
      "Loss:  1.1267510652542114\n",
      "Loss:  0.755530595779419\n",
      "Loss:  0.7022801637649536\n",
      "Loss:  0.7050411701202393\n",
      "Loss:  0.6948195099830627\n",
      "Loss:  0.6919822692871094\n",
      "Loss:  0.6903543472290039\n",
      "Loss:  0.688232421875\n",
      "Loss:  0.6853128671646118\n",
      "Loss:  0.6812276244163513\n",
      "Loss:  0.6757012605667114\n",
      "Loss:  0.6686353087425232\n",
      "Loss:  0.6600804328918457\n",
      "Loss:  0.6502411365509033\n",
      "Loss:  0.6394853591918945\n",
      "Loss:  0.6282865405082703\n",
      "Loss:  0.6170278191566467\n",
      "Loss:  0.605910062789917\n",
      "Loss:  0.5950087904930115\n",
      "Loss:  0.5843648314476013\n",
      "Loss:  0.5740143060684204\n",
      "Loss:  0.5639845728874207\n",
      "Loss:  0.5542863607406616\n",
      "Loss:  0.544917106628418\n",
      "Loss:  0.535876989364624\n",
      "Loss:  0.5271687507629395\n",
      "Loss:  0.5187905430793762\n",
      "Loss:  0.5107370018959045\n",
      "Loss:  0.5029990077018738\n",
      "Loss:  0.49556460976600647\n",
      "Loss:  0.4884180426597595\n",
      "Loss:  0.4815470576286316\n",
      "Loss:  0.47494131326675415\n",
      "Loss:  0.4685870110988617\n",
      "Loss:  0.46247100830078125\n",
      "Loss:  0.4565817713737488\n",
      "Loss:  0.45090755820274353\n",
      "Loss:  0.44543734192848206\n",
      "Loss:  0.4401627480983734\n",
      "Loss:  0.43507635593414307\n",
      "Loss:  0.43017256259918213\n",
      "Loss:  0.42544248700141907\n",
      "Loss:  0.42087608575820923\n",
      "Loss:  0.41646450757980347\n",
      "Loss:  0.41220054030418396\n",
      "Loss:  0.4080776572227478\n",
      "Loss:  0.4040888249874115\n",
      "Loss:  0.4002273380756378\n",
      "Loss:  0.39648720622062683\n",
      "Loss:  0.3928631842136383\n",
      "Loss:  0.3893497586250305\n",
      "Loss:  0.3859409987926483\n",
      "Loss:  0.3826310932636261\n",
      "Loss:  0.37941429018974304\n",
      "Loss:  0.3762858211994171\n",
      "Loss:  0.3732409179210663\n",
      "Loss:  0.37027421593666077\n",
      "Loss:  0.36738038063049316\n",
      "Loss:  0.36455461382865906\n",
      "Loss:  0.3617926836013794\n",
      "Loss:  0.35909131169319153\n",
      "Loss:  0.35644739866256714\n",
      "Loss:  0.35385826230049133\n",
      "Loss:  0.3513216972351074\n",
      "Loss:  0.3488355278968811\n",
      "Loss:  0.34639787673950195\n",
      "Loss:  0.3440069556236267\n",
      "Loss:  0.34166136384010315\n",
      "Loss:  0.3393601179122925\n",
      "Loss:  0.33710262179374695\n",
      "Loss:  0.33488842844963074\n",
      "Loss:  0.3327164053916931\n",
      "Loss:  0.3305855393409729\n",
      "Loss:  0.32849451899528503\n",
      "Loss:  0.32644224166870117\n",
      "Loss:  0.3244272768497467\n",
      "Loss:  0.322448194026947\n",
      "Loss:  0.3205035328865051\n",
      "Loss:  0.3185917139053345\n",
      "Loss:  0.31671109795570374\n",
      "Loss:  0.3148600459098816\n",
      "Loss:  0.3130369484424591\n",
      "Loss:  0.3112402856349945\n",
      "Loss:  0.30946841835975647\n",
      "Loss:  0.3077196776866913\n",
      "Loss:  0.30599260330200195\n",
      "Loss:  0.30428579449653625\n",
      "Loss:  0.30259814858436584\n",
      "Loss:  0.3009289503097534\n",
      "Loss:  0.29927781224250793\n",
      "Loss:  0.2976444959640503\n",
      "Loss:  0.2960284948348999\n",
      "Loss:  0.29442939162254333\n",
      "Loss:  0.2928464710712433\n",
      "Loss:  0.29127925634384155\n",
      "Loss:  0.28972718119621277\n",
      "Loss:  0.28818976879119873\n",
      "Loss:  0.2866666316986084\n",
      "Loss:  0.2851574122905731\n",
      "Loss:  0.28366175293922424\n",
      "training layer:  1\n",
      "Loss:  1.1267026662826538\n",
      "Loss:  0.6372690200805664\n",
      "Loss:  0.5515586733818054\n",
      "Loss:  0.5382932424545288\n",
      "Loss:  0.5332889556884766\n",
      "Loss:  0.5214734077453613\n",
      "Loss:  0.5119490623474121\n",
      "Loss:  0.49998417496681213\n",
      "Loss:  0.4855197072029114\n",
      "Loss:  0.46902552247047424\n",
      "Loss:  0.45166733860969543\n",
      "Loss:  0.4347067177295685\n",
      "Loss:  0.41923099756240845\n",
      "Loss:  0.40568113327026367\n",
      "Loss:  0.39386388659477234\n",
      "Loss:  0.3833700716495514\n",
      "Loss:  0.3739089071750641\n",
      "Loss:  0.3654283583164215\n",
      "Loss:  0.3577497601509094\n",
      "Loss:  0.3507203757762909\n",
      "Loss:  0.34431764483451843\n",
      "Loss:  0.33846545219421387\n",
      "Loss:  0.33307042717933655\n",
      "Loss:  0.32806533575057983\n",
      "Loss:  0.32338184118270874\n",
      "Loss:  0.31899818778038025\n",
      "Loss:  0.31489455699920654\n",
      "Loss:  0.3110295534133911\n",
      "Loss:  0.30738529562950134\n",
      "Loss:  0.3039374351501465\n",
      "Loss:  0.30066153407096863\n",
      "Loss:  0.297542542219162\n",
      "Loss:  0.2945686876773834\n",
      "Loss:  0.29172515869140625\n",
      "Loss:  0.28899818658828735\n",
      "Loss:  0.2863743007183075\n",
      "Loss:  0.2838437259197235\n",
      "Loss:  0.2814023792743683\n",
      "Loss:  0.2790464460849762\n",
      "Loss:  0.27677229046821594\n",
      "Loss:  0.27457568049430847\n",
      "Loss:  0.2724517285823822\n",
      "Loss:  0.27039581537246704\n",
      "Loss:  0.2684037685394287\n",
      "Loss:  0.2664717435836792\n",
      "Loss:  0.264596164226532\n",
      "Loss:  0.2627740800380707\n",
      "Loss:  0.2610027492046356\n",
      "Loss:  0.2592793405056\n",
      "Loss:  0.25760123133659363\n",
      "Loss:  0.25596627593040466\n",
      "Loss:  0.2543727457523346\n",
      "Loss:  0.2528190314769745\n",
      "Loss:  0.2513037919998169\n",
      "Loss:  0.24982582032680511\n",
      "Loss:  0.2483833134174347\n",
      "Loss:  0.24697473645210266\n",
      "Loss:  0.24559864401817322\n",
      "Loss:  0.24425378441810608\n",
      "Loss:  0.24293896555900574\n",
      "Loss:  0.24165286123752594\n",
      "Loss:  0.24039381742477417\n",
      "Loss:  0.2391607016324997\n",
      "Loss:  0.2379533350467682\n",
      "Loss:  0.2367713749408722\n",
      "Loss:  0.23561394214630127\n",
      "Loss:  0.23447997868061066\n",
      "Loss:  0.23336859047412872\n",
      "Loss:  0.232278972864151\n",
      "Loss:  0.2312103807926178\n",
      "Loss:  0.23016206920146942\n",
      "Loss:  0.22913339734077454\n",
      "Loss:  0.2281237095594406\n",
      "Loss:  0.2271323800086975\n",
      "Loss:  0.22615882754325867\n",
      "Loss:  0.22520245611667633\n",
      "Loss:  0.22426271438598633\n",
      "Loss:  0.22333897650241852\n",
      "Loss:  0.22243069112300873\n",
      "Loss:  0.22153732180595398\n",
      "Loss:  0.22065819799900055\n",
      "Loss:  0.21979272365570068\n",
      "Loss:  0.21894016861915588\n",
      "Loss:  0.2180996686220169\n",
      "Loss:  0.21727007627487183\n",
      "Loss:  0.21644999086856842\n",
      "Loss:  0.2156379371881485\n",
      "Loss:  0.21483482420444489\n",
      "Loss:  0.21404363214969635\n",
      "Loss:  0.21326585114002228\n",
      "Loss:  0.21250151097774506\n",
      "Loss:  0.2117498368024826\n",
      "Loss:  0.21100980043411255\n",
      "Loss:  0.21028056740760803\n",
      "Loss:  0.20956134796142578\n",
      "Loss:  0.20885136723518372\n",
      "Loss:  0.20814988017082214\n",
      "Loss:  0.2074560523033142\n",
      "Loss:  0.20676875114440918\n",
      "Loss:  0.2060871422290802\n",
      "train error: 0.07700002193450928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnet.predict(x) = tensor([0, 9, 7, ...., 8, 7, 4])\\n\\nsay Actual labels are y = tensor([0, 9, 1, ...., 8, 1, 5])])\\n\\nnet.predict(x).eq(y) = tensor([True, True, False, ...., True, False, False])\\n\\nTrue becomes 1.0 and False becomes 0.0.\\nnet.predict(x).eq(y).float() = tensor([1.0, 1.0, 0.0, ...., 1.0, 0.0, 0.0])\\n\\nTake average of the tensor\\nnet.predict(x).eq(y).float().mean() = tensor([0.6667])\\n\\nError rate = 1 - accuracy\\n1 - net.predict(x).eq(y).float().mean().item() = 1 - 0.6667 \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is a 2D tensor of size (batch_size, 784) and y is a 1D tensor of size (batch_size,).\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "# Move the input data and labels to the specified device (CPU or GPU).\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# overlay_y_on_x() function returns a 2D tensor of size (batch_size, 784)\n",
    "x_pos = overlay_y_on_x(x, y)\n",
    "\n",
    "# get_y_neg() function returns a 1D tensor of size (batch_size,) with the negative labels.\n",
    "y_neg = get_y_neg(y)\n",
    "\n",
    "# overlay_y_on_x() function returns a 2D tensor of size (batch_size, 784)\n",
    "x_neg = overlay_y_on_x(x, y_neg)\n",
    "\n",
    "# Train the network with the positive and negative examples.\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "# Predict the labels for the training data, compare them to the actual labels, and calculate the error rate.\n",
    "print(\"train error:\", 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "'''\n",
    "net.predict(x) = tensor([0, 9, 7, ...., 8, 7, 4])\n",
    "\n",
    "say Actual labels are y = tensor([0, 9, 1, ...., 8, 1, 5])])\n",
    "\n",
    "net.predict(x).eq(y) = tensor([True, True, False, ...., True, False, False])\n",
    "\n",
    "True becomes 1.0 and False becomes 0.0.\n",
    "net.predict(x).eq(y).float() = tensor([1.0, 1.0, 0.0, ...., 1.0, 0.0, 0.0])\n",
    "\n",
    "Take average of the tensor\n",
    "net.predict(x).eq(y).float().mean() = tensor([0.6667])\n",
    "\n",
    "Error rate = 1 - accuracy\n",
    "1 - net.predict(x).eq(y).float().mean().item() = 1 - 0.6667 \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4fb4f",
   "metadata": {},
   "source": [
    "### 4. Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7e7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: 0.07109999656677246\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch of the test data.\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "\n",
    "# Move the test data and labels to the specified device (CPU or GPU).\n",
    "x_te, y_te = x_te.to(device), y_te.to(device)\n",
    "\n",
    "# If the 'save_model' argument is true, save the current state of the trained network.\n",
    "if args.save_model:\n",
    "    torch.save(net.state_dict(), \"mnist_ff.pt\")\n",
    "\n",
    "# Predict the labels for the test data, compare them to the actual labels, and calculate the error rate.\n",
    "print(\"test error:\", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab69987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
